{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BASE_DATA_PATH = os.path.join(\"out\")\n",
    "OUTPUT_FOLDER = os.path.join(\"out\", \"plots\")\n",
    "\n",
    "def get_dataset_file(cloud_name, timestamp = \"latest\", data_path = BASE_DATA_PATH):\n",
    "    # Get all CSV files in the folder\n",
    "    csv_folder = os.path.join(data_path, cloud_name)\n",
    "    csv_files = glob.glob(os.path.join(csv_folder, \"*.csv\"))\n",
    "    df = None\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in the folder: {csv_folder}\")\n",
    "    \n",
    "    if timestamp == 'latest':\n",
    "        # Parse filenames and find the latest based on the timestamp in the name\n",
    "        file = max(csv_files, key=lambda x: datetime.strptime(\n",
    "            '-'.join(x.split('-')[1:]).replace('.csv', ''),\n",
    "            \"%Y-%m-%d-%H:%M:%S\"\n",
    "        ))\n",
    "        print(f\"Loading latest file: {file}\")\n",
    "        df = pd.read_csv(file)\n",
    "    else:\n",
    "        # Check for exact match with the date_str in the filename (ignoring the prefix)\n",
    "        for file in csv_files:\n",
    "            filename = os.path.basename(file)\n",
    "            file_timestamp = filename.split('-')[1:]  # Split to get timestamp part\n",
    "            file_timestamp = '-'.join(file_timestamp).replace('.csv', '')  # Rebuild timestamp string\n",
    "            if timestamp == file_timestamp:\n",
    "                print(f\"Loading file: {file}\")\n",
    "                df = pd.read_csv(file)\n",
    "        if df is None:\n",
    "               FileNotFoundError(f\"File with date '{timestamp}' not found in folder: {csv_folder}\")\n",
    "    # Convert times to milliseconds\n",
    "    df['mean'] = df['mean'] * 1000\n",
    "    df['stdev'] = df['stdev'] * 1000# \n",
    "    df['warmup_time'] = df['warmup_time'] * 1000\n",
    "    return df\n",
    "\n",
    "def read_multiple_datasets(clouds_datasets, data_path = BASE_DATA_PATH):\n",
    "    dfs = {}\n",
    "    for cloud, dataset in clouds_datasets.items():\n",
    "        dfs[cloud] = get_dataset_file(cloud, \"latest\", data_path)\n",
    "    return dfs\n",
    "\n",
    "def output_fig(fig, filename, dataset = \"all\", cloud = None):\n",
    "    output_folder = \"\"\n",
    "    if cloud is None:\n",
    "        output_folder = os.path.join(OUTPUT_FOLDER, dataset)\n",
    "    else:\n",
    "        output_folder = os.path.join(OUTPUT_FOLDER, dataset, cloud)\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    fig.savefig(os.path.join(output_folder, filename), dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'font.family': 'sans-serif',  # Use sans-serif as a fallback\n",
    "    'font.sans-serif': ['Arial', 'Helvetica', 'DejaVu Sans'],\n",
    "    \n",
    "    'font.size': 10,\n",
    "    'axes.labelsize': 9,\n",
    "    'axes.titlesize': 11,\n",
    "    'xtick.labelsize': 8,\n",
    "    'ytick.labelsize': 8,\n",
    "    'legend.fontsize': 8,\n",
    "    'figure.titlesize': 12,\n",
    "    \n",
    "    'axes.grid': True,\n",
    "    'axes.grid.axis': 'y',  # Only horizontal grid lines\n",
    "    'grid.linestyle': '-',  # Solid line\n",
    "    'grid.linewidth': 0.4,  # Very thin grid lines\n",
    "    'grid.color': '#CCCCCC',  # Light gray grid\n",
    "    \n",
    "    'axes.axisbelow': True,\n",
    "    'figure.figsize': (6, 4),  # Standard publication-friendly figure size\n",
    "    'figure.dpi': 100, # Smaller preview\n",
    "    # 'figure.dpi': 300, \n",
    "    \n",
    "    'lines.linewidth': 1.0,  # Consistent line thickness\n",
    "    'lines.markersize': 4,  # Consistent marker size\n",
    "\n",
    "    'figure.facecolor': 'white',  # Background color of the figure\n",
    "    'figure.edgecolor': 'white'   # Edge color of the figure\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some aux. functions for plot information\n",
    "def add_execution_details(cloud, dataset, searches, repeats, points, fig, h_ex=0.93):\n",
    "    execution_details = [\n",
    "        f\"- {searches:,} searches x {repeats:,} repeats\",\n",
    "        f\"- Point cloud: {cloud} ({points:,} points)\",\n",
    "        f\"- Dataset: {dataset}\",\n",
    "    ]\n",
    "    fig.text(0.10, h_ex,\n",
    "            '\\n'.join(execution_details),\n",
    "            fontfamily='monospace',\n",
    "            color='#505050',\n",
    "            ha='left',\n",
    "            va='top',\n",
    "            linespacing=1.3)\n",
    "    \n",
    "def add_execution_details_multiple_datasets(clouds, searches, repeats, fig, h_ex=0.90):\n",
    "    execution_details = [\n",
    "        f\"- {searches:,} searches x {repeats:,} repeats\",\n",
    "        f\"- {len(clouds)} point clouds\",\n",
    "    ]\n",
    "    fig.text(0.10, h_ex,\n",
    "            '\\n'.join(execution_details),\n",
    "            fontfamily='monospace',\n",
    "            color='#505050',\n",
    "            ha='left',\n",
    "            va='top',\n",
    "            linespacing=1.3)\n",
    "\n",
    "def add_title_subtitle(title, subtitle, fig, h_title=0.98, h_subtitle=0.95):\n",
    "    fig.text(0.10, h_title, \n",
    "            title, \n",
    "            fontsize=16, \n",
    "            fontweight='bold', \n",
    "            ha='left', \n",
    "            va='top')\n",
    "\n",
    "    fig.text(0.10, h_subtitle,\n",
    "            subtitle,\n",
    "            fontsize=12,\n",
    "            fontstyle='italic',\n",
    "            color='#404040',\n",
    "            ha='left',\n",
    "            va='top')\n",
    "\n",
    "def add_octree_types_legend(legend_handles, legend_labels, legend_title, fig):\n",
    "    fig.legend(\n",
    "        legend_handles,\n",
    "        legend_labels,\n",
    "        title=legend_title,\n",
    "        loc=\"upper right\",\n",
    "        bbox_to_anchor=(0.9, 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dicts store information about the type parameters and possible combinations used in each visualization, \n",
    "# along with the palette of colors\n",
    "TYPES_INFO_OCTREE_ENCODER = {\n",
    "    \"type_parameters\":  ['octree', 'encoder'],\n",
    "    \"available_types\": pd.DataFrame({\n",
    "        'octree': ['LinearOctree', 'LinearOctree', 'Octree', 'Octree', 'Octree'],\n",
    "        'encoder': ['HilbertEncoder3D', 'MortonEncoder3D', 'HilbertEncoder3D', 'MortonEncoder3D', 'Unencoded']\n",
    "    }),\n",
    "\n",
    "    \"palette\": {\n",
    "        ('LinearOctree', 'HilbertEncoder3D'): '#1984c5',\n",
    "        ('LinearOctree', 'MortonEncoder3D'): '#63bff0',\n",
    "        ('Octree', 'HilbertEncoder3D'): '#c23728',\n",
    "        ('Octree', 'MortonEncoder3D'): '#de6e56',\n",
    "        ('Octree', 'Unencoded'): '#e1a692'\n",
    "    }\n",
    "}\n",
    "\n",
    "TYPES_INFO_OCTREE_POINT = {\n",
    "    \"type_parameters\":  ['octree', 'point_type'],\n",
    "    \"available_types\": pd.DataFrame({\n",
    "        'octree': ['LinearOctree', 'LinearOctree', 'LinearOctree', 'Octree', 'Octree', 'Octree'],\n",
    "        'point_type': ['Point', 'Lpoint64', 'Lpoint', 'Point', 'Lpoint64', 'Lpoint']\n",
    "    }),\n",
    "    \"palette\": {\n",
    "        ('LinearOctree', 'Point'): '#0f5f87',\n",
    "        ('LinearOctree', 'Lpoint64'): '#1984c5',\n",
    "        ('LinearOctree', 'Lpoint'): '#63bff0',\n",
    "        ('Octree', 'Point'): '#9f1b17',\n",
    "        ('Octree', 'Lpoint64'): '#de6e56',\n",
    "        ('Octree', 'Lpoint'): '#e1a692'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OCTREE_COMP_DATA_PATH = os.path.join(BASE_DATA_PATH, \"octree_comp\")\n",
    "ALGO_COMP_DATA_PATH = os.path.join(BASE_DATA_PATH, \"algo_comp\")\n",
    "POINT_COMP_DATA_PATH = os.path.join(BASE_DATA_PATH, \"point_comp\")\n",
    "APPROX_SEARCH_DATA_PATH = os.path.join(BASE_DATA_PATH, \"approx_search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tpp plots\n",
    "TPP_DATA_PATH = os.path.join(\"out_old\", \"tpp_comp\")\n",
    "CLOUDS_DATASETS_TPP = {\"alcoy\": \"alcoy\",\n",
    "                        \"Lille_0\": \"Paris_Lille\", \n",
    "                        \"5110_54320\": \"Dales_LAS\",\n",
    "                        \"5135_54435\": \"Dales_LAS\",\n",
    "                        \"bildstein_station1_xyz_intensity_rgb\": \"Semantic3D\",\n",
    "                        \"sg27_station8_intensity_rgb\": \"Semantic3D\",\n",
    "                        \"station1_xyz_intensity_rgb\": \"Semantic3D\",\n",
    "                        \"Speulderbos_2017_TLS\": \"Speulderbos\"\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideas para plots de multiples datasets:\n",
    "# 1. Fijar kernel, radio (al máximo por ejemplo) y una operación. Iterar para cada dataframe e implementación del octree.\n",
    "# Plottear tamaño del dataset vs tiempo de ejecución / avg_result_size. En cada punto del line graph debería estar el nombre del dataset para que sea fácil interpretarlo. \n",
    "# Plottear una línea para cada implementación del octree.\n",
    "# Los tamaños de los datasets están ordenados y están en forma de log plot en el eje x.\n",
    "# Se debería ver como un dataset más grande hace que la eficiencia por punto encontrado sea peor, ya que habrá más fallos de caché y más overheads.\n",
    "# El problema es poder escoger buen radio/kernel para que todos los datasets tengan un avg_result_size similar, ya que si es muy distinto no se podrá comparar bien,\n",
    "# cuando el avg_result_size es muy pequeño, el tiempo por punto encontrado es más alto por los overheads. Así que hice varios métodos, uno de ellos es fijar el radio,\n",
    "# otro escoger el radio de cada dataset que tiene el avg_result_size más grande y el último es escoger el radio que se acerca más a un valor target (avg_size_target).\n",
    "# Los 2 últimos métodos eligen distintos radios para cada dataframe, y el último es el más interesante en mi opinión.\n",
    "\n",
    "# 2. Fijar kernel y para cada radio (no todos estarán disponibles en todos los datasets por densidades diferentes), imprimir cada uno de los runtimes\n",
    "# para cada tipo de octree. Es el más sencillo pero no es independiente de la densidad de puntos. Es la generalización directa de los plots de antes y al contrario\n",
    "# que el anterior no muestra como afecta el tamaño total del dataset a la eficiencia.\n",
    "\n",
    "# Otra idea TODO: Fijar un kernel y una operacion, iterar por cada dataset, radio e implementación del octree\n",
    "# Plottear el avg_result_size para cada par (dataset, radio) vs el tiempo de ejecución para cada implementación del octree. Una línea en un line graph\n",
    "# por cada implementación del octree. Los avg_result_sizes están ordenados por tamaño, obviamente mayor radio => mayor avg_result_size, pero no necesariamente\n",
    "# a través de datasets, porque la densidad varía. El problema que le veo a esta idea es que no ves cómo afecta el tamaño del dataset en el rendimiento, ya que no se\n",
    "# plotea el dataset de cada avg_result_size, así que solo es una extensión de los plots que se podrían hacer con 1 solo dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea 1.\n",
    "\n",
    "# tpp = time per point found (ms/point)\n",
    "# This one is not used right now because I did not do an execution with all datasets having same radius\n",
    "def tpp_fixed_radius(clouds_datasets, kernel, radius, operation, operation_name, types_info=TYPES_INFO_OCTREE_ENCODER, data_path=TPP_DATA_PATH):\n",
    "    dfs = read_multiple_datasets(clouds_datasets, data_path)\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    implementation_data = {}\n",
    "    legend_handles, legend_labels = [], []\n",
    "    x_labels, x_ticks = [], []\n",
    "    for df_name, df in dfs.items():\n",
    "        df = df[(df['kernel'] == kernel) & (df['radius'] == radius) & (df['operation'] == operation)]        \n",
    "        df = df.reset_index(drop=True) \n",
    "        avg_result_size = df['avg_result_size'].iloc[0]\n",
    "        dataset_size = df['npoints'].iloc[0]\n",
    "        x_labels.append(f\"{df_name}\\nN = {dataset_size}\")\n",
    "        x_ticks.append(dataset_size)\n",
    "        for j, (_, params) in enumerate(types_info[\"available_types\"].iterrows()):\n",
    "            key = tuple(params[col] for col in types_info[\"type_parameters\"])\n",
    "            df_data = df[\n",
    "                (df[types_info[\"type_parameters\"]] == pd.Series(key, index=types_info[\"type_parameters\"])).all(axis=1)\n",
    "            ]\n",
    "            if df_data.empty:\n",
    "                print(\"Warning, no data for \", key, \" with kernel = \", kernel, \n",
    "                      \", radius = \", radius, \" and operation = \", operation, \" at dataset \", df_name)\n",
    "                \n",
    "            norm_time = df_data['mean'].iloc[0] / avg_result_size # ms / point\n",
    "            if key not in implementation_data:\n",
    "                implementation_data[key] = {'sizes': [], 'times': []}\n",
    "            implementation_data[key]['sizes'].append(dataset_size)\n",
    "            implementation_data[key]['times'].append(norm_time)\n",
    "\n",
    "   # Plot a line for each implementation\n",
    "    for key, data in implementation_data.items():\n",
    "        # Sort by size to ensure proper line connection\n",
    "        sizes = np.array(data['sizes'])\n",
    "        times = np.array(data['times'])\n",
    "        sort_idx = np.argsort(sizes)\n",
    "        \n",
    "        formatted_label = \", \".join(f\"{value}\" for value in key)\n",
    "        line = ax.plot(sizes[sort_idx], times[sort_idx], 'o-', \n",
    "                      label=formatted_label, \n",
    "                      color=types_info[\"palette\"][key],\n",
    "                      linewidth=2, markersize=8)[0]\n",
    "        \n",
    "\n",
    "        if formatted_label not in legend_labels:\n",
    "            legend_handles.append(line)\n",
    "            legend_labels.append(formatted_label)\n",
    "    \n",
    "    ax.set_xscale('log')\n",
    "    \n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
    "\n",
    "    ax.set_ylabel('Average time per point found (ms/point)', fontsize=12)\n",
    "    \n",
    "    ax.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    \n",
    "    add_octree_types_legend(legend_handles, legend_labels, \"Octree type\", fig)\n",
    "    add_title_subtitle(\n",
    "        f\"{operation_name} performance analysis\",\n",
    "        f\"Average time per point found using {kernel} kernel and radius {radius}\",\n",
    "        fig,\n",
    "        h_title = 0.98,\n",
    "        h_subtitle  = 0.93\n",
    "    )\n",
    "    \n",
    "    nsearches = df['num_searches'].iloc[0]\n",
    "    nrepeats = df['repeats'].iloc[0]\n",
    "    add_execution_details_multiple_datasets(dfs.keys(), nsearches, nrepeats, fig, h_ex = 0.88)\n",
    "    \n",
    "    plt.subplots_adjust(right=0.85, top=0.75)  # Make room for legend and title\n",
    "\n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the biggest avg_result_size instead of fixing kernel,radius so datasets with different densities \n",
    "# all have a lot of points found in the search\n",
    "\n",
    "def tpp_max_avg_result_size(clouds_datasets, kernel, operation, operation_name, types_info=TYPES_INFO_OCTREE_ENCODER, data_path=TPP_DATA_PATH):\n",
    "    dfs = read_multiple_datasets(clouds_datasets, data_path)\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    implementation_data = {}\n",
    "    legend_handles, legend_labels = [], []\n",
    "    x_labels, x_ticks = [], []\n",
    "\n",
    "    for df_name, df in dfs.items():\n",
    "        # Filter the dataframe by kernel and operation\n",
    "        df = df[(df['kernel'] == kernel) & (df['operation'] == operation)]\n",
    "        # Need to reset the index so we can use idxmax\n",
    "        df = df.reset_index(drop=True) \n",
    "\n",
    "        # Find the maximum row and get its maximum\n",
    "        max_row = df.loc[df['avg_result_size'].idxmax()]\n",
    "        avg_result_size, radius, dataset_size = max_row['avg_result_size'], max_row['radius'], max_row['npoints']\n",
    "\n",
    "        print(f\"Dataset: {df_name}, radius taken: {radius}, max avg_result_size: {avg_result_size}\")\n",
    "\n",
    "        x_labels.append(f\"{df_name}\\nN = {dataset_size}\")\n",
    "        x_ticks.append(dataset_size)\n",
    "\n",
    "        for j, (_, params) in enumerate(types_info[\"available_types\"].iterrows()):\n",
    "            key = tuple(params[col] for col in types_info[\"type_parameters\"])\n",
    "            df_data = df[\n",
    "                (df[types_info[\"type_parameters\"]] == pd.Series(key, index=types_info[\"type_parameters\"])).all(axis=1)\n",
    "            ]\n",
    "            if df_data.empty:\n",
    "                print(\"Warning, no data for \", key, \" with kernel = \", kernel, \n",
    "                      \", radius = \", radius, \" and operation = \", operation, \" at dataset \", df_name)\n",
    "                \n",
    "            norm_time = df_data['mean'].iloc[0] / avg_result_size  # ms / point\n",
    "            if key not in implementation_data:\n",
    "                implementation_data[key] = {'sizes': [], 'times': []}\n",
    "            implementation_data[key]['sizes'].append(dataset_size)\n",
    "            implementation_data[key]['times'].append(norm_time)\n",
    "\n",
    "    # Plot a line for each implementation\n",
    "    for key, data in implementation_data.items():\n",
    "        # Sort by size to ensure proper line connection\n",
    "        sizes = np.array(data['sizes'])\n",
    "        times = np.array(data['times'])\n",
    "        sort_idx = np.argsort(sizes)\n",
    "        \n",
    "        formatted_label = \", \".join(f\"{value}\" for value in key)\n",
    "        line = ax.plot(sizes[sort_idx], times[sort_idx], 'o-', \n",
    "                      label=formatted_label, \n",
    "                      color=types_info[\"palette\"][key],\n",
    "                      linewidth=2, markersize=8)[0]\n",
    "        \n",
    "        if formatted_label not in legend_labels:\n",
    "            legend_handles.append(line)\n",
    "            legend_labels.append(formatted_label)\n",
    "    \n",
    "    ax.set_xscale('log')\n",
    "    \n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
    "\n",
    "    ax.set_ylabel('Average time per point found (ms/point)', fontsize=12)\n",
    "    \n",
    "    ax.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    \n",
    "    add_octree_types_legend(legend_handles, legend_labels, \"Octree type\", fig)\n",
    "    add_title_subtitle(\n",
    "        f\"{operation_name} performance analysis\",\n",
    "        f\"Average time per point found using {kernel} kernel, using maximum radius available\",\n",
    "        fig,\n",
    "        h_title = 0.98,\n",
    "        h_subtitle  = 0.93\n",
    "    )\n",
    "    \n",
    "    nsearches = max_row['num_searches']\n",
    "    nrepeats = max_row['repeats']\n",
    "    add_execution_details_multiple_datasets(dfs.keys(), nsearches, nrepeats, fig, h_ex = 0.88)\n",
    "    \n",
    "    plt.subplots_adjust(right=0.85, top=0.75)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = tpp_max_avg_result_size(\n",
    "    CLOUDS_DATASETS_TPP,\n",
    "    kernel=\"Sphere\",\n",
    "    operation='neighSearch',\n",
    "    operation_name='Neighbor Search'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the biggest avg_result_size instead of fixing kernel,radius so datasets with different densities \n",
    "# all have a lot of points found\n",
    "\n",
    "def tpp_closest_to_target_avg(clouds_datasets, kernel, target_avg, operation, operation_name, types_info=TYPES_INFO_OCTREE_ENCODER, data_path=TPP_DATA_PATH):\n",
    "    dfs = read_multiple_datasets(clouds_datasets, data_path)\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    implementation_data = {}\n",
    "    legend_handles, legend_labels = [], []\n",
    "    x_labels, x_ticks = [], []\n",
    "\n",
    "    for df_name, df in dfs.items():\n",
    "        # Filter the dataframe by kernel and operation\n",
    "        df = df[(df['kernel'] == kernel) & (df['operation'] == operation)]\n",
    "        # Need to reset the index so we can use idxmax\n",
    "        df = df.reset_index(drop=True) \n",
    "\n",
    "        # Find the row with avg_result_size closest to the target_avg_max\n",
    "        closest_row = df.iloc[(df['avg_result_size'] - target_avg).abs().idxmin()]\n",
    "        # Dataset size could have been taken from any row, but we take it from the closest row for convenience\n",
    "        avg_size_found, chosen_radius, dataset_size = closest_row['avg_result_size'], closest_row['radius'], closest_row[\"npoints\"]\n",
    "        print(f\"Dataset: {df_name}, radius chosen: {chosen_radius}, found avg_result_size: {avg_size_found}, with diff. to target: {abs(avg_size_found - target_avg)}\")\n",
    "\n",
    "\n",
    "        x_labels.append(f\"{df_name}\\nN = {dataset_size}\")\n",
    "        x_ticks.append(dataset_size)\n",
    "\n",
    "        for j, (_, params) in enumerate(types_info[\"available_types\"].iterrows()):\n",
    "            key = tuple(params[col] for col in types_info[\"type_parameters\"])\n",
    "            # Filter the dataframe by octree type\n",
    "            octree_impl_df = df[\n",
    "                (df[types_info[\"type_parameters\"]] == pd.Series(key, index=types_info[\"type_parameters\"])).all(axis=1)\n",
    "            ]\n",
    "            if octree_impl_df.empty:\n",
    "                print(\"Warning, no data for \", key, \" with kernel = \", kernel, \n",
    "                      \", radius = \", chosen_radius, \" and operation = \", operation, \" at dataset \", df_name)\n",
    "                \n",
    "            norm_time = octree_impl_df['mean'].iloc[0] / avg_size_found  # ms / point\n",
    "            if key not in implementation_data:\n",
    "                implementation_data[key] = {'sizes': [], 'times': []}\n",
    "            implementation_data[key]['sizes'].append(dataset_size)\n",
    "            implementation_data[key]['times'].append(norm_time)\n",
    "\n",
    "    # Plot a line for each implementation\n",
    "    for key, data in implementation_data.items():\n",
    "        # Sort by size to ensure proper line connection\n",
    "        sizes = np.array(data['sizes'])\n",
    "        times = np.array(data['times'])\n",
    "        sort_idx = np.argsort(sizes)\n",
    "        \n",
    "        formatted_label = \", \".join(f\"{value}\" for value in key)\n",
    "        line = ax.plot(sizes[sort_idx], times[sort_idx], 'o-', \n",
    "                      label=formatted_label, \n",
    "                      color=types_info[\"palette\"][key],\n",
    "                      linewidth=2, markersize=8)[0]\n",
    "        \n",
    "        if formatted_label not in legend_labels:\n",
    "            legend_handles.append(line)\n",
    "            legend_labels.append(formatted_label)\n",
    "    \n",
    "    ax.set_xscale('log')\n",
    "    \n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
    "\n",
    "    ax.set_ylabel('Average time per point found (ms/point)', fontsize=12)\n",
    "    \n",
    "    ax.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    \n",
    "    add_octree_types_legend(legend_handles, legend_labels, \"Octree type\", fig)\n",
    "    add_title_subtitle(\n",
    "        f\"{operation_name} performance analysis\",\n",
    "        f\"Analysis of average time to find points over multiple datasets using {kernel} kernel\\n\"\n",
    "        f\"Radius taken is the one making avg. result size closest to {target_avg}\",\n",
    "        fig,\n",
    "        h_title = 0.98,\n",
    "        h_subtitle  = 0.93\n",
    "    )\n",
    "\n",
    "    # This values should be the same for all datasets, otherwise data is inconsistent\n",
    "    nsearches = closest_row['num_searches']\n",
    "    nrepeats = closest_row['repeats']\n",
    "    add_execution_details_multiple_datasets(dfs.keys(), nsearches, nrepeats, fig, h_ex = 0.86)\n",
    "    \n",
    "    plt.subplots_adjust(right=0.85, top=0.75)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = tpp_closest_to_target_avg(\n",
    "    CLOUDS_DATASETS_TPP,\n",
    "    kernel=\"Sphere\",\n",
    "    target_avg=30000,\n",
    "    operation='neighSearch',\n",
    "    operation_name='Neighbor Search'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fig(tpp_max_avg_result_size(CLOUDS_DATASETS_TPP, kernel=\"Sphere\", operation='neighSearch', operation_name='Neighbors Search'), \"tpp_neigh_search_max_radius\")\n",
    "output_fig(tpp_max_avg_result_size(CLOUDS_DATASETS_TPP, kernel=\"Sphere\", operation='numNeighSearch', operation_name='Num. of Neighbors Search'), \"tpp_num_neigh_search_max_radius\")\n",
    "TARGET_AVG = 30000\n",
    "output_fig(tpp_closest_to_target_avg(CLOUDS_DATASETS_TPP, kernel=\"Sphere\", target_avg=TARGET_AVG, operation='neighSearch', operation_name='Neighbors Search'), f\"tpp_num_neigh_search_avg_near_{TARGET_AVG}\")\n",
    "output_fig(tpp_closest_to_target_avg(CLOUDS_DATASETS_TPP, kernel=\"Sphere\", target_avg=TARGET_AVG, operation='numNeighSearch', operation_name='Num. of Neighbors Search'),f\"tpp_num_neigh_search_avg_near_{TARGET_AVG}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
